"use strict";(self.webpackChunkfoss_guru=self.webpackChunkfoss_guru||[]).push([[8e3],{7681:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>l,toc:()=>d});const l=JSON.parse('{"id":"kits/spring-ai/Getting Started/setup-ollama","title":"Ollama Local Setup and Spring AI Integration","description":"Learn to download, install, and run an LLM model using Ollama. Also learn to configure Spring AI Ollama module to access the model\u2019s chat API.","source":"@site/community/kits/spring-ai/Getting Started/setup-ollama.md","sourceDirName":"kits/spring-ai/Getting Started","slug":"/kits/spring-ai/Getting Started/setup-ollama","permalink":"/foss-guru/community/kits/spring-ai/Getting Started/setup-ollama","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"setup-ollama","title":"Ollama Local Setup and Spring AI Integration","description":"Learn to download, install, and run an LLM model using Ollama. Also learn to configure Spring AI Ollama module to access the model\u2019s chat API.","sidebar_position":4},"sidebar":"community","previous":{"title":"Spring AI Structured Output Converters","permalink":"/foss-guru/community/kits/spring-ai/Getting Started/structured-output"},"next":{"title":"Models","permalink":"/foss-guru/community/category/models"}}');var s=a(4848),t=a(8453);const i={id:"setup-ollama",title:"Ollama Local Setup and Spring AI Integration",description:"Learn to download, install, and run an LLM model using Ollama. Also learn to configure Spring AI Ollama module to access the model\u2019s chat API.",sidebar_position:4},r=void 0,o={},d=[{value:"\ud83d\ude80 Mastering Ollama: Install, Run &amp; Connect with Spring AI \ud83e\udd16",id:"-mastering-ollama-install-run--connect-with-spring-ai-",level:2},{value:"\ud83e\uddd0 What is Ollama?",id:"-what-is-ollama",level:2},{value:"\ud83d\udd27 Installing Ollama",id:"-installing-ollama",level:2},{value:"1\ufe0f\u20e3 One-Click Installer (Easiest!) \ud83d\uddb1\ufe0f",id:"1\ufe0f\u20e3-one-click-installer-easiest-\ufe0f",level:3},{value:"2\ufe0f\u20e3 Command Line Installation (For the Techies) \ud83d\udda5\ufe0f",id:"2\ufe0f\u20e3-command-line-installation-for-the-techies-\ufe0f",level:3},{value:"3\ufe0f\u20e3 Running Ollama in Docker (For the Container Wizards) \ud83d\udc33",id:"3\ufe0f\u20e3-running-ollama-in-docker-for-the-container-wizards-",level:3},{value:"\ud83e\udd16 Download &amp; Run a Model",id:"-download--run-a-model",level:2},{value:"\ud83d\udd0c Connecting to Ollama API (AI in Your Apps!)",id:"-connecting-to-ollama-api-ai-in-your-apps",level:2},{value:"\u26a1 Using Ollama with Spring AI",id:"-using-ollama-with-spring-ai",level:2},{value:"\ud83d\udce6 1. Add Maven Dependency",id:"-1-add-maven-dependency",level:3},{value:"\ud83d\udd27 2. Configure Base URL &amp; Model Name",id:"-2-configure-base-url--model-name",level:3},{value:"\ud83d\udcac 3. Send AI Prompts &amp; Get Responses",id:"-3-send-ai-prompts--get-responses",level:3},{value:"\ud83c\udfaf Wrapping Up",id:"-wrapping-up",level:2}];function c(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Banner Spring AI Icon",src:a(9666).A+"",width:"1022",height:"239"})}),"\n",(0,s.jsx)(n.h2,{id:"-mastering-ollama-install-run--connect-with-spring-ai-",children:"\ud83d\ude80 Mastering Ollama: Install, Run & Connect with Spring AI \ud83e\udd16"}),"\n",(0,s.jsx)(n.p,{children:"Want to bring AI magic to your local machine? Buckle up! This guide will walk you through installing Ollama, running powerful LLMs, and connecting them with Spring AI. It's like OpenAI's GPT, but you run it yourself! Let's dive in! \ud83c\udf89"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-what-is-ollama",children:"\ud83e\uddd0 What is Ollama?"}),"\n",(0,s.jsxs)(n.p,{children:["Ollama is your AI sidekick that lets you run large language models (LLMs) right on your computer. Think of it as Docker, but for AI models! \ud83d\udee0\ufe0f Instead of spinning up databases or message queues, Ollama lets you effortlessly download, install, and chat with AI models like ",(0,s.jsx)(n.strong,{children:"LLaMA-2, Mistral, CodeLLaMA and DeepSeek-R1"}),". You can even fine-tune them for your needs. Cool, right? \ud83d\ude0e"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-installing-ollama",children:"\ud83d\udd27 Installing Ollama"}),"\n",(0,s.jsxs)(n.p,{children:["Ollama is easy to set up, and you have ",(0,s.jsx)(n.strong,{children:"three ways"})," to get it running."]}),"\n",(0,s.jsx)(n.h3,{id:"1\ufe0f\u20e3-one-click-installer-easiest-\ufe0f",children:"1\ufe0f\u20e3 One-Click Installer (Easiest!) \ud83d\uddb1\ufe0f"}),"\n",(0,s.jsx)(n.p,{children:"For Windows & Mac users who love simplicity."}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Steps:"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Visit ",(0,s.jsx)(n.a,{href:"https://ollama.com/",children:"Ollama's website"})," \ud83c\udf0d"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Smash that ",(0,s.jsx)(n.strong,{children:"Download"})," button \ud83d\udce5"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run the installer (.exe for Windows, .dmg for Mac)"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Follow the on-screen setup wizard \ud83e\uddd9"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Once installed, fire it up with:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"ollama serve\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"\ud83c\udf89 Done! Easy, right?"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"2\ufe0f\u20e3-command-line-installation-for-the-techies-\ufe0f",children:"2\ufe0f\u20e3 Command Line Installation (For the Techies) \ud83d\udda5\ufe0f"}),"\n",(0,s.jsx)(n.p,{children:"Perfect for Linux users who love typing commands. \ud83d\udcbb"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"curl -fsSL https://ollama.com/install.sh | sh\nollama serve\n"})}),"\n",(0,s.jsx)(n.p,{children:"\ud83d\ude80 Boom! You're up and running."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"3\ufe0f\u20e3-running-ollama-in-docker-for-the-container-wizards-",children:"3\ufe0f\u20e3 Running Ollama in Docker (For the Container Wizards) \ud83d\udc33"}),"\n",(0,s.jsx)(n.p,{children:"Want to isolate Ollama in a container? Here's how:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"docker pull ollama/ollama\ndocker run -d --gpus all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n"})}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udcdd If you have multiple GPUs, replace ",(0,s.jsx)(n.code,{children:"all"})," with the specific GPU ID."]}),"\n",(0,s.jsx)(n.p,{children:"Start it with:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"docker start ollama\nollama serve\n"})}),"\n",(0,s.jsx)(n.p,{children:"And you're ready to roll! \ud83c\udfa2"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-download--run-a-model",children:"\ud83e\udd16 Download & Run a Model"}),"\n",(0,s.jsx)(n.p,{children:"Time to get your hands dirty! Let's grab a powerful AI model and chat with it. \ud83d\udde3\ufe0f"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"ollama run gemma2\n"})}),"\n",(0,s.jsxs)(n.p,{children:["This command ",(0,s.jsx)(n.strong,{children:"downloads"})," the Gemma2 model (if you don't have it already) and gets it ",(0,s.jsx)(n.strong,{children:"ready for action"}),". Just type your prompts, and let AI do the magic! \u2728"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-connecting-to-ollama-api-ai-in-your-apps",children:"\ud83d\udd0c Connecting to Ollama API (AI in Your Apps!)"}),"\n",(0,s.jsxs)(n.p,{children:["Ollama provides a REST API at ",(0,s.jsx)(n.code,{children:"http://localhost:11434"}),", making it super easy to integrate with your apps."]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udca1 ",(0,s.jsx)(n.strong,{children:"Try this out!"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:'curl http://localhost:11434/api/generate -d \'{\n  "model": "llama3",\n  "prompt": "Why is the sky blue?"\n}\'\n'})}),"\n",(0,s.jsx)(n.p,{children:"\ud83c\udf0d You can also use Postman or any HTTP client to test it."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-using-ollama-with-spring-ai",children:"\u26a1 Using Ollama with Spring AI"}),"\n",(0,s.jsx)(n.p,{children:"Just like OpenAI, Spring AI makes it easy to talk to Ollama. Here's how you can integrate it into your Java project."}),"\n",(0,s.jsx)(n.h3,{id:"-1-add-maven-dependency",children:"\ud83d\udce6 1. Add Maven Dependency"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"<dependency>\n  <groupId>org.springframework.ai</groupId>\n  <artifactId>spring-ai-ollama-spring-boot-starter</artifactId>\n</dependency>\n"})}),"\n",(0,s.jsx)(n.h3,{id:"-2-configure-base-url--model-name",children:"\ud83d\udd27 2. Configure Base URL & Model Name"}),"\n",(0,s.jsxs)(n.p,{children:["By default, Spring AI assumes ",(0,s.jsx)(n.strong,{children:"localhost:11434"})," with model ",(0,s.jsx)(n.code,{children:"mistral"}),". You can tweak it like this:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-properties",children:"spring.ai.ollama.base-url=http://localhost:11434\nspring.ai.ollama.chat.options.model=gemma\nspring.ai.ollama.chat.options.temperature=0.4\n"})}),"\n",(0,s.jsx)(n.p,{children:"Or in Java:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'@Bean\nOllamaChatModel ollamaChatModel(@Value("spring.ai.ollama.base-url") String baseUrl) {\n  return new OllamaChatModel(new OllamaApi(baseUrl),\n    OllamaOptions.create()\n      .withModel("gemma")\n      .withTemperature(0.4f));\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"-3-send-ai-prompts--get-responses",children:"\ud83d\udcac 3. Send AI Prompts & Get Responses"}),"\n",(0,s.jsx)(n.p,{children:"Want to chat with your AI model? Easy!"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'@Autowired\nOllamaChatModel chatModel;\n\nchatModel.stream(new Prompt(\n    "Generate the names of 5 famous pirates.",\n    OllamaOptions.create()\n      .withModel("gemma2")\n      .withTemperature(0.4F)\n)).subscribe(chatResponse -> {\n  System.out.print(chatResponse.getResult().getOutput().getContent());\n});\n'})}),"\n",(0,s.jsx)(n.p,{children:"Or use synchronous calls:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'ChatResponse response = chatModel.call(\n  new Prompt(\n    "Generate the names of 5 famous pirates.",\n    OllamaOptions.create()\n      .withModel("gemma2")\n      .withTemperature(0.4F)\n  ));\n\nresponse.getResults().stream()\n  .map(generation -> generation.getOutput().getContent())\n  .forEach(System.out::println);\n'})}),"\n",(0,s.jsx)(n.p,{children:"\ud83d\ude80 Now your Spring Boot app can talk to an AI model just like ChatGPT!"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-wrapping-up",children:"\ud83c\udfaf Wrapping Up"}),"\n",(0,s.jsxs)(n.p,{children:["In this guide, we:\n\u2705 Installed Ollama in multiple ways \ud83d\udee0\ufe0f\n\u2705 Downloaded & ran an AI model \ud83e\udd16\n\u2705 Connected to the Ollama API \ud83d\udd0c\n\u2705 Integrated it with ",(0,s.jsx)(n.strong,{children:"Spring AI"})," \ud83d\ude80"]}),"\n",(0,s.jsxs)(n.p,{children:["Ollama is a ",(0,s.jsx)(n.strong,{children:"game-changer"})," for local AI development. Whether you're building chatbots, AI-powered apps, or fine-tuning models, ",(0,s.jsx)(n.strong,{children:"you're now ready to conquer the AI world!"})," \ud83c\udf0e\ud83d\udca1"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Happy Coding! \ud83d\ude80\ud83d\udd25"})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},9666:(e,n,a)=>{a.d(n,{A:()=>l});const l=a.p+"assets/images/spring_ai_logo-85ebfad6246caa6a26060f054d26ebc8.png"},8453:(e,n,a)=>{a.d(n,{R:()=>i,x:()=>r});var l=a(6540);const s={},t=l.createContext(s);function i(e){const n=l.useContext(t);return l.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),l.createElement(t.Provider,{value:n},e.children)}}}]);